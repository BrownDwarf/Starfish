This figure (posteriors.png) shows the log-normal priors and gaussian priors from test_priors.py. The take-away message is that using a log-normal prior on c_0 means that the peak of the posterior distribution will not be biased, ie, it will be at the correct location. Increasing sigma from 0.2 to 1 makes the effect more apparent, yet for the sigma=1, the LN is still peaked at the correct value (10), in this case the solid line marks the mean of the ensemble of fD values, drawn with a sigma_f=2.

posteriors2.png shows this even better for the intermediate case of sigmac=0.6. People are going to draw mean and confidence estimates off of the peak and surrounding "1 sigma" regions of the distribution. If we used the Gaussian priors, we would get a region that is systematically less than F=10, wheras if we use the log-normal priors, we always get F=10 with some wide region regardless of what sigmac is.
